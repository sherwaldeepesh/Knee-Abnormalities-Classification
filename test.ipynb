{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.1+cu117', '0.15.2+cu117')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch.utils.data as data\n",
    "\n",
    "torch.__version__,torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ncullen93/torchsample\n",
      "  Cloning https://github.com/ncullen93/torchsample to c:\\users\\deepe\\appdata\\local\\temp\\pip-req-build-ihx902_j\n",
      "  Resolved https://github.com/ncullen93/torchsample to commit 1f328d1ea3ef533c8c0c4097ed4a3fa16d784ba4\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/ncullen93/torchsample 'C:\\Users\\deepe\\AppData\\Local\\Temp\\pip-req-build-ihx902_j'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\deepe\\anaconda3\\envs\\gpuaml\\lib\\site-packages (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ncullen93/torchsample\n",
    "!pip install einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = 'data/MRNet-v1.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops, math\n",
    "\n",
    "def patchify(x,ps=4):\n",
    "    return einops.rearrange(x,\"b c (h p1) (w p2)-> b (h w) (p1 p2 c)\",p1=ps,p2=ps)\n",
    "\n",
    "def unpatchify(x,ps=4):\n",
    "    return einops.rearrange(x,\"b (h w) (p1 p2 c) -> b c (h p1) (w p2)\",p1=ps,p2=ps, h=int(math.sqrt(x.shape[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementation\n",
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    def norm_cdf(x):\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.requires_attn = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x) # B, N, 3, self.num_heads x C // self.num_heads\n",
    "        if self.requires_attn:\n",
    "            qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2] # 1, B, self.num_heads, N, C // self.num_heads\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "\n",
    "            x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        else:\n",
    "            qkv = qkv.reshape(B, N, 3,C)\n",
    "            x = qkv[:,:,2]\n",
    "            attn = None\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'embeded patch {x.shape}')\n",
    "        B, C, H, W = x.shape\n",
    "        # print(x.shape)\n",
    "        ####################### write your answer here #######################\n",
    "        # instruction: you need to project features from patch pixels with 3xpatch_sizexpatch_size to a token with embed_dim\n",
    "        \n",
    "        ####################### write your answer here ####################### \n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(2*embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "    \n",
    "    def prepare_tokens(self, x):\n",
    "        # print(f\"prepare token shape {x.shape}\")\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        B,N,C = x.shape\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        ####################### write your answer here ####################### \n",
    "        # instruction: you need to add cls_tokens to the sequence. After this, the shape of x should be (B, N+1, C)\n",
    "        ####################### write your answer here ####################### \n",
    "        # assert x.shape[1] == N + 1\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "    \n",
    "    def forward_features(self, x):\n",
    "        # print(f'first {x.shape}')\n",
    "        x = torch.squeeze(x, dim=0)\n",
    "        # print(f'second {x.shape}')\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "                \n",
    "        return self.head( torch.cat( (x[:, 0], torch.mean(x[:, 1:], dim=1)), dim=1 ) ) \n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=4, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "def vit_little(patch_size=4, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=64, depth=4, num_heads=4, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "class MRDataset(data.Dataset):\n",
    "    def __init__(self, root_dir, task, plane, train=True, transform=None, weights=None):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.plane = plane\n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            self.folder_path = self.root_dir + 'train/{0}/'.format(plane)\n",
    "            self.records = pd.read_csv(\n",
    "                self.root_dir + 'train-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
    "        else:\n",
    "            transform = None\n",
    "            self.folder_path = self.root_dir + 'valid/{0}/'.format(plane)\n",
    "            self.records = pd.read_csv(\n",
    "                self.root_dir + 'valid-{0}.csv'.format(task), header=None, names=['id', 'label'])\n",
    "\n",
    "        self.records['id'] = self.records['id'].map(\n",
    "            lambda i: '0' * (4 - len(str(i))) + str(i))\n",
    "        self.paths = [self.folder_path + filename +\n",
    "                      '.npy' for filename in self.records['id'].tolist()]\n",
    "        self.labels = self.records['label'].tolist()\n",
    "\n",
    "        self.transform = transform\n",
    "        if weights is None:\n",
    "            pos = np.sum(self.labels)\n",
    "            neg = len(self.labels) - pos\n",
    "            self.weights = [1, neg / pos]\n",
    "        else:\n",
    "            self.weights = weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        array = np.load(self.paths[index])[5]\n",
    "        # array = np.expand_dims(array, axis=0)\n",
    "        # array = np.stack((array,)*3, axis=1)[0]\n",
    "        # array = np.stack((array,)*3, axis=1)\n",
    "        # slice_index = 5\n",
    "        # axis = 0\n",
    "        # slice_2d = array.take(indices=slice_index, axis=axis)\n",
    "\n",
    "        # array = Image.fromarray(slice_2d).resize((224, 224), Image.BILINEAR)\n",
    "        # array = np.array(array)\n",
    "        # array = np.expand_dims(array, axis=2)\n",
    "        # array = np.repeat(array, 1, axis=2)\n",
    "\n",
    "        # print(array.shape)\n",
    "        label = self.labels[index]\n",
    "        label = torch.FloatTensor([label])\n",
    "        \n",
    "        if self.transform:\n",
    "            array = self.transform(array)\n",
    "        else:\n",
    "            array = np.stack((array,)*3, axis=0)\n",
    "            array = torch.FloatTensor(array)\n",
    "            # print(f'final array shape {array.shape}')\n",
    "            # array = transforms.RandomResizedCrop(array, scale=(0.2, 1.0), interpolation=3)\n",
    "\n",
    "        if label.item() == 1:\n",
    "            weight = np.array([self.weights[1]])\n",
    "            weight = torch.FloatTensor(weight)\n",
    "        else:\n",
    "            weight = np.array([self.weights[0]])\n",
    "            weight = torch.FloatTensor(weight)\n",
    "\n",
    "        return array, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchsample.transforms import RandomRotate, RandomTranslate, RandomFlip, Compose\n",
    "from torchvision import transforms\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['module.head.mlp.0.weight', 'module.head.mlp.0.bias', 'module.head.mlp.1.weight', 'module.head.mlp.1.bias', 'module.head.mlp.1.running_mean', 'module.head.mlp.1.running_var', 'module.head.mlp.1.num_batches_tracked', 'module.head.mlp.3.weight', 'module.head.mlp.3.bias', 'module.head.mlp.4.weight', 'module.head.mlp.4.bias', 'module.head.mlp.4.running_mean', 'module.head.mlp.4.running_var', 'module.head.mlp.4.num_batches_tracked', 'module.head.mlp.6.weight', 'module.head.mlp.6.bias', 'module.head.mlp.7.running_mean', 'module.head.mlp.7.running_var', 'module.head.mlp.7.num_batches_tracked', 'module.head_recons.mlp.0.weight', 'module.head_recons.mlp.0.bias', 'module.head_recons.mlp.2.weight', 'module.head_recons.mlp.2.bias', 'module.head_recons.mlp.4.weight', 'module.head_recons.mlp.4.bias', 'module.head_recons.convTrans.weight', 'module.head_recons.convTrans.bias'])\n"
     ]
    }
   ],
   "source": [
    "# now load a train set and a validation set\n",
    "# data = datasets.CIFAR10(\"data\",transform=tfms,download=True, )\n",
    "# train_dl = torch.utils.data.DataLoader(data,256,shuffle=True,num_workers=8,pin_memory=True)\n",
    "\n",
    "task = 'meniscus'\n",
    "plane = 'axial'\n",
    "\n",
    "augmentor = Compose([\n",
    "        transforms.Lambda(lambda x: torch.Tensor(x)),\n",
    "        # RandomRotate(25),\n",
    "        RandomTranslate([0.11, 0.11]),\n",
    "        RandomFlip(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1, 1).permute(1, 0, 2, 3)),\n",
    "    ])\n",
    "\n",
    "# train_dataset = MRDataset(root_directory, task,\n",
    "#                               plane, transform=augmentor, train=True)\n",
    "\n",
    "train_dataset = MRDataset(root_directory, task,\n",
    "                              plane, train=True)\n",
    "# train_dl = torch.utils.data.DataLoader(\n",
    "        # train_dataset, batch_size=1, shuffle=True, drop_last=False)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "         train_dataset, batch_size=32, shuffle=True, drop_last=False)\n",
    "\n",
    "\n",
    "# data = datasets.CIFAR10(\"data\",False,transform=tfms,download=True)\n",
    "# val_dl = torch.utils.data.DataLoader(data,64,shuffle=False,num_workers=4)\n",
    "\n",
    "validation_dataset = MRDataset(\n",
    "        root_directory, task, plane, train=False)\n",
    "# val_dl = torch.utils.data.DataLoader(\n",
    "#     validation_dataset, batch_size=1, shuffle=-True, num_workers=4, drop_last=False)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(\n",
    "     validation_dataset, batch_size=3, shuffle=-True, drop_last=False)\n",
    "\n",
    "#  Create Model\n",
    "device = \"cuda\"\n",
    "model = vit_small(16,img_size=224,num_classes=1)\n",
    "model = model.to(device)\n",
    "\n",
    "# load the pretrained weights\n",
    "state = torch.load(\"SiT_Small_ImageNet.pth\")\n",
    "state = {k.replace(\"module.backbone.\",\"\"):v for k,v in state['teacher'].items()} # extract the model weights!\n",
    "print(\"Loading weights: \", model.load_state_dict(state,strict=False))\n",
    "\n",
    "# 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "from torch import optim\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# torch.squeeze(next(iter(train_dl))[0].float(), dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train_one_epoch(dl,model, opt, device):\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(dl)\n",
    "    for x,label in pbar:\n",
    "        x,label = x.float().to(device),label.to(device)\n",
    "        pred = model(x).float()\n",
    "        # pred = torch.argmax(pred, 1).unsqueeze(1)\n",
    "        # print(label)\n",
    "        # softmax = nn.Softmax(dim=1)\n",
    "        # pred = softmax(pred)\n",
    "        # loss = nn.functional.cross_entropy(pred,label)\n",
    "        \n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        loss = loss(pred,label)\n",
    "        # loss = nn.MSELoss(pred,label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        pbar.set_description(f\"loss: {loss.item():.3f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(dl,model, device):\n",
    "    model.eval()\n",
    "    pbar = tqdm.tqdm(dl)\n",
    "    def _f():\n",
    "        for x,label in pbar:\n",
    "            x,label = x.to(device),label.to(device)\n",
    "            pred = model(x).float()\n",
    "            # pred = torch.argmax(pred, 1).unsqueeze(1)\n",
    "            # softmax = nn.Softmax(dim=1)\n",
    "            # pred = softmax(pred)\n",
    "            # loss = nn.functional.cross_entropy(pred,label)\n",
    "            \n",
    "            loss = nn.BCEWithLogitsLoss()\n",
    "            loss = loss(pred,label)\n",
    "            # loss = nn.MSELoss(pred,label)\n",
    "            acc = (pred.argmax(1)==label).float().mean()\n",
    "            pbar.set_description(f\"loss: {loss.item():.3f}, acc: {acc.item():.3f}\")\n",
    "            yield pred.argmax(1),label\n",
    "    out = list(_f())\n",
    "    pred, y = list(zip(*out))\n",
    "    return torch.cat(pred),torch.cat(y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.689: 100%|██████████| 36/36 [00:13<00:00,  2.69it/s]\n",
      "loss: 0.696, acc: 0.667: 100%|██████████| 40/40 [00:01<00:00, 30.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5666666626930237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.648: 100%|██████████| 36/36 [00:11<00:00,  3.07it/s]\n",
      "loss: 0.726: 100%|██████████| 36/36 [00:11<00:00,  3.20it/s]\n",
      "loss: 0.719: 100%|██████████| 36/36 [00:11<00:00,  3.09it/s]\n",
      "loss: 0.556: 100%|██████████| 36/36 [00:11<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# this implementation achieves roughly an accuracy of 0.64.\n",
    "num_epoch = 5\n",
    "log=[]\n",
    "for epoch in range(num_epoch):\n",
    "    train_one_epoch(train_dl,model, optimizer, device)\n",
    "    if epoch%10==0:\n",
    "        pred,y = eval_one_epoch(val_dl,model, device)\n",
    "        acc = (pred==y).float().mean().item()        \n",
    "        log.append(acc)\n",
    "        print(epoch, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# pred,y = eval_one_epoch(val_dl,model, device)\n",
    "cf_max=confusion_matrix(pred.cpu(),y.cpu())\n",
    "sns.heatmap(cf_max,annot=True,fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "healthcoursework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
